{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da1cd1c5",
   "metadata": {},
   "source": [
    "# MVSA-Single Text Sentiment Classification (TF‑IDF + Logistic Regression)\n",
    "### Preprocessing Ablation Study (Raw vs. Individual Steps vs. Full Pipeline)\n",
    "\n",
    "This notebook implements a **text-only** sentiment classification framework on the **MVSA-Single** dataset. It conducts a **controlled ablation study** to measure how specific preprocessing steps impact the performance of a **Logistic Regression** classifier.\n",
    "\n",
    "## Objectives\n",
    "- **Model Baseline**: Establish a baseline using a Logistic Regression classifier on raw text.\n",
    "- **Ablation Study**: Compare **Raw text** against **individual preprocessing steps** and a **Full Preprocessing Pipeline** on the same English subset.\n",
    "- **Performance Metrics**: Evaluate improvements using **Accuracy** and **Macro-F1** score (chosen for its robustness in class-imbalanced scenarios).\n",
    "\n",
    "## Experimental Design\n",
    "All experiments maintain consistent control variables:\n",
    "- **Dataset**: Identical English subset.\n",
    "- **Split**: Stratified Train/Test split with a fixed random seed.\n",
    "- **Features**: Consistent TF-IDF vectorization settings.\n",
    "- **Model**: Fixed Logistic Regression hyperparameters.\n",
    "\n",
    "We isolate and vary a **single factor** in each experiment (with the exception of the **Full** pipeline, which aggregates all methods).\n",
    "\n",
    "|   ID   | Description                                           | Text Transform | Training Strategy         |\n",
    "|:------:|-------------------------------------------------------|----------------|---------------------------|\n",
    "| **E0** | Baseline (Raw English subset)                         | None           | None                      |\n",
    "| **E1** | Noise Removal (Remove `@/#/url`)                      | ✅              | None                      |\n",
    "| **E2** | Normalization (Slang expansion)                       | ✅              | None                      |\n",
    "| **E3** | Class Balancing                                       | None           | `class_weight=\"balanced\"` |\n",
    "| **E4** | **FULL Pipeline** (Noise Removal + Slang + Balancing) | ✅              | `class_weight=\"balanced\"` |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "id": "2c182f4f",
   "metadata": {},
   "source": [
    "# =========================\n",
    "# 1) Setup & Imports\n",
    "# =========================\n",
    "# This notebook is designed to be reproducible:\n",
    "# - Fixed random seed for splitting\n",
    "# - Deterministic language detection seed (if langdetect is installed)\n",
    "# - All key parameters are defined in one place\n",
    "\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Dict, Callable, Optional, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "\n",
    "# Reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Make langdetect deterministic if available\n",
    "try:\n",
    "    from langdetect import DetectorFactory\n",
    "    DetectorFactory.seed = 0\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "print(\"Imports complete.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ea36b191",
   "metadata": {},
   "source": [
    "## 2) Load MVSA-Single Dataset (Text + Sentiment Labels)\n",
    "\n",
    "The dataset is sourced from [Kaggle: MVSA-Single](https://www.kaggle.com/datasets/vincemarcs/mvsasingle).\n",
    "\n",
    "The MVSA-Single dataset structure typically consists of:\n",
    "- `labelResultAll.txt`: A tab-separated file containing labels for each sample ID (format: `text_sentiment, image_sentiment`).\n",
    "- `data/<ID>.txt`: Individual text files containing the raw tweet content.\n",
    "\n",
    "**Data Loading Pipeline:**\n",
    "1.  **Download**: Retrieve the dataset programmatically.\n",
    "2.  **Parse Labels**: Extract the **text sentiment** label (the first value in the `text,image` pair).\n",
    "3.  **Read Content**: Ingest tweet texts from the `data/*.txt` files.\n",
    "4.  **Merge**: Consolidate data into a single DataFrame `(ID, text, label)`.\n",
    "\n",
    "> **Note**: This project focuses exclusively on **text-based** sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "id": "ca342344",
   "metadata": {},
   "source": [
    "# =========================\n",
    "# 2) Dataset Extraction\n",
    "# =========================\n",
    "\n",
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"vincemarcs/mvsasingle\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "MVSA_ROOT = Path(os.path.join(path, \"MVSA_Single\"))\n",
    "LABEL_FILE = Path(os.path.join(MVSA_ROOT, \"labelResultAll.txt\"))\n",
    "DATA_DIR = Path(os.path.join(MVSA_ROOT, \"data\"))\n",
    "\n",
    "print(\"MVSA_ROOT:\", MVSA_ROOT)\n",
    "print(\"LABEL_FILE:\", LABEL_FILE)\n",
    "print(\"DATA_DIR:\", DATA_DIR)\n",
    "\n",
    "# Load label file: columns are typically [\"ID\", \"text,image\"]\n",
    "labels = pd.read_csv(LABEL_FILE, sep=\"\\t\", engine=\"python\")\n",
    "labels.columns = [c.strip() for c in labels.columns]\n",
    "\n",
    "# Extract *text* sentiment label from \"text,image\" column\n",
    "# Example row: \"neutral,positive\"  -> text label = \"neutral\"\n",
    "labels[\"label\"] = labels[\"text,image\"].astype(str).str.split(\",\").str[0].str.strip().str.lower()\n",
    "labels[\"ID\"] = labels[\"ID\"].astype(int)\n",
    "labels = labels.dropna(subset=[\"label\"])\n",
    "\n",
    "# Load texts from data/*.txt\n",
    "rows = []\n",
    "id_pattern = re.compile(r\"^(\\d+)\\.txt$\")\n",
    "\n",
    "for p in DATA_DIR.glob(\"*.txt\"):\n",
    "    m = id_pattern.match(p.name)\n",
    "    if not m:\n",
    "        continue\n",
    "    _id = int(m.group(1))\n",
    "    # Tweets can contain emojis or unusual chars; ignore decoding errors safely\n",
    "    text = p.read_text(encoding=\"utf-8\", errors=\"ignore\").strip()\n",
    "    rows.append((_id, text))\n",
    "\n",
    "texts = pd.DataFrame(rows, columns=[\"ID\", \"text\"])\n",
    "\n",
    "# Merge text + label\n",
    "df = texts.merge(labels[[\"ID\", \"label\"]], on=\"ID\", how=\"inner\")\n",
    "df = df[[\"text\", \"label\"]].dropna().reset_index(drop=True)\n",
    "\n",
    "print(\"Merged dataset shape:\", df.shape)\n",
    "df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d2621e83",
   "metadata": {},
   "source": [
    "## 3) Exploratory Data Analysis (EDA)\n",
    "\n",
    "We perform a brief analysis to inspect:\n",
    "- **Class Distribution**: Identifying potential class imbalance.\n",
    "- **Sample Quality**: Reviewing random samples to understand noise levels (e.g., mentions, hashtags, URLs, slang).\n",
    "- **Text Length Analysis**: Investigating whether the length of the text correlates with specific sentiment labels (e.g., are negative tweets longer?).\n",
    "\n",
    "This step is crucial for determining the necessity of specific preprocessing techniques and understanding feature separability."
   ]
  },
  {
   "cell_type": "code",
   "id": "56867644",
   "metadata": {},
   "source": [
    "# Label distribution\n",
    "label_counts = df[\"label\"].value_counts()\n",
    "label_dist = (label_counts / len(df)).round(4)\n",
    "\n",
    "display(pd.DataFrame({\"count\": label_counts, \"ratio\": label_dist}))\n",
    "\n",
    "# Show a few random examples\n",
    "df.sample(5, random_state=RANDOM_STATE)\n",
    "\n",
    "# Text Length Analysis\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "temp_df = df.copy()\n",
    "temp_df['text_length'] = temp_df['text'].apply(str).apply(len)\n",
    "le = LabelEncoder()\n",
    "temp_df['target_code'] = le.fit_transform(temp_df['label'])\n",
    "\n",
    "corr = temp_df[['text_length', 'target_code']].corr()\n",
    "print(corr)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Word Cloud Visualization\n",
    "To intuitively understand the distinctive vocabulary associated with each sentiment, we generate **Word Clouds** for the *Negative*, *Neutral*, and *Positive* classes. This visualization highlights the most frequent tokens, allowing us to qualitatively inspect whether specific keywords (e.g., adjectives, topic-specific terms) are strongly correlated with ground-truth labels."
   ],
   "id": "d98ec1f578e1a2a0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "all_neutral_words = ' '.join(df[df['label'] == 'neutral']['text'].str.lower())\n",
    "all_positive_words = ' '.join(df[df['label'] == 'positive']['text'].str.lower())\n",
    "all_negative_words = ' '.join(df[df['label'] == 'negative']['text'].str.lower())\n",
    "\n",
    "def plt_word_cloud(all_word):\n",
    "  wordcloud = WordCloud(width=800, height=400,\n",
    "                      background_color='white',\n",
    "                      min_font_size=10).generate(all_word)\n",
    "\n",
    "  # 4. Display the generated image\n",
    "  plt.figure(figsize=(10, 5))\n",
    "  plt.imshow(wordcloud, interpolation='bilinear')\n",
    "  plt.axis(\"off\") # Hide the axes\n",
    "  plt.show()\n",
    "\n",
    "print(\"negative:\")\n",
    "plt_word_cloud(all_negative_words)\n",
    "print(\"neutral:\")\n",
    "plt_word_cloud(all_neutral_words)\n",
    "print(\"positive:\")\n",
    "plt_word_cloud(all_positive_words)"
   ],
   "id": "a3ea26b2dcef73f4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4) English Subset Filtering\n",
    "\n",
    "The MVSA-Single dataset is multilingual. To ensure experimental validity and align with project requirements, we filter the dataset to retain only an **English subset**. All subsequent ablation experiments are performed on this filtered subset.\n",
    "\n",
    "**Filtering Logic:**\n",
    "- **Primary Method**: Use `langdetect` (if installed) for robust language identification.\n",
    "- **Fallback Method**: Use a heuristic based on **ASCII character ratio** if `langdetect` is unavailable."
   ],
   "id": "40b12f7ddcf40c22"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# =========================\n",
    "# 4) English Detection Helpers\n",
    "# =========================\n",
    "\n",
    "_whitespace_re = re.compile(r\"\\s+\")\n",
    "\n",
    "def normalize_text_basic(text: str) -> str:\n",
    "    # Minimal normalization shared by all experiments:\n",
    "    # - Replace line breaks with spaces\n",
    "    # - Collapse repeated whitespace\n",
    "    # - Trim leading/trailing spaces\n",
    "    text = \"\" if text is None else str(text)\n",
    "    text = text.replace(\"\\r\", \" \").replace(\"\\n\", \" \")\n",
    "    text = _whitespace_re.sub(\" \", text).strip()\n",
    "    return text\n",
    "\n",
    "def is_english_langdetect(text: str) -> Optional[bool]:\n",
    "    # Return True/False using langdetect if available.\n",
    "    # Return None if langdetect is not installed.\n",
    "    try:\n",
    "        from langdetect import detect\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "    t = normalize_text_basic(text)\n",
    "    if len(t) < 3:\n",
    "        return False\n",
    "    try:\n",
    "        return detect(t) == \"en\"\n",
    "    except Exception:\n",
    "        # langdetect can fail on very short or noisy strings\n",
    "        return False\n",
    "\n",
    "def is_english_ascii_heuristic(text: str, threshold: float = 0.90) -> bool:\n",
    "    # Fallback heuristic:\n",
    "    # - Count the fraction of ASCII characters\n",
    "    # - If ratio >= threshold, treat as English-like\n",
    "    t = normalize_text_basic(text)\n",
    "    if not t:\n",
    "        return False\n",
    "    ascii_chars = sum(1 for c in t if ord(c) < 128)\n",
    "    return (ascii_chars / max(1, len(t))) >= threshold\n",
    "\n",
    "def is_english(text: str) -> bool:\n",
    "    r = is_english_langdetect(text)\n",
    "    if r is None:\n",
    "        return is_english_ascii_heuristic(text)\n",
    "    return bool(r)\n",
    "\n",
    "# Apply English filtering\n",
    "df[\"is_english\"] = df[\"text\"].apply(is_english)\n",
    "df_en = df.loc[df[\"is_english\"]].drop(columns=[\"is_english\"]).reset_index(drop=True)\n",
    "\n",
    "print(\"Before English filter:\", len(df))\n",
    "print(\"After English filter:\", len(df_en))\n",
    "\n",
    "display(df_en[\"label\"].value_counts())"
   ],
   "id": "be956f00be811419",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5) Preprocessing Modules\n",
    "\n",
    "We implement modular preprocessing functions to be used selectively in the ablation experiments:\n",
    "\n",
    "### (A) Noise Removal (Social Tokens)\n",
    "Removes platform-specific artifacts that may introduce noise:\n",
    "- `@mentions` (Usernames)\n",
    "- `#hashtags` (Topics)\n",
    "- `http/https` (URLs)\n",
    "\n",
    "### (B) Slang Expansion (Normalization)\n",
    "Normalizes non-standard internet slang into standard English using a predefined dictionary mapping:\n",
    "- *Example*: `lol` $\\rightarrow$ `laugh out loud`, `idk` $\\rightarrow$ `i do not know`.\n",
    "\n",
    "### (C) Full Pipeline (E4)\n",
    "A composite pipeline that applies:\n",
    "1.  Social token removal.\n",
    "2.  Slang expansion.\n",
    "3.  (During Training) Class weighting."
   ],
   "id": "2a68e952bd3a01c9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# =========================\n",
    "# 5) Preprocessing Functions\n",
    "# =========================\n",
    "\n",
    "# Token pattern: sequences of non-space characters\n",
    "_token_re = re.compile(r\"\\b\\S+\\b\", flags=re.UNICODE)\n",
    "\n",
    "def remove_social_tokens(text: str) -> str:\n",
    "    # Remove tokens that start with '@', '#', or 'http'.\n",
    "    # This is common for tweet-like data where mentions/hashtags/URLs can add noise.\n",
    "    t = normalize_text_basic(text)\n",
    "    tokens = _token_re.findall(t)\n",
    "    kept = []\n",
    "    for tok in tokens:\n",
    "        low = tok.lower()\n",
    "        if low.startswith(\"@\") or low.startswith(\"#\") or low.startswith(\"http\"):\n",
    "            continue\n",
    "        kept.append(tok)\n",
    "    return \" \".join(kept)\n",
    "\n",
    "# A small, editable slang dictionary (you can extend this list as needed)\n",
    "SLANG_DICT: Dict[str, str] = {\n",
    "    \"lol\": \"laugh out loud\",\n",
    "    \"lmao\": \"laughing my ass off\",\n",
    "    \"rofl\": \"rolling on the floor laughing\",\n",
    "    \"idk\": \"i do not know\",\n",
    "    \"imo\": \"in my opinion\",\n",
    "    \"imho\": \"in my humble opinion\",\n",
    "    \"omg\": \"oh my god\",\n",
    "    \"btw\": \"by the way\",\n",
    "    \"brb\": \"be right back\",\n",
    "    \"ttyl\": \"talk to you later\",\n",
    "    \"thx\": \"thanks\",\n",
    "    \"u\": \"you\",\n",
    "    \"ur\": \"your\",\n",
    "    \"pls\": \"please\",\n",
    "    \"plz\": \"please\",\n",
    "    \"w/\": \"with\",\n",
    "    \"w/o\": \"without\",\n",
    "}\n",
    "\n",
    "# Lowercase keys for case-insensitive matching\n",
    "SLANG_DICT = {k.lower(): v for k, v in SLANG_DICT.items()}\n",
    "\n",
    "def expand_slang(text: str, slang: Dict[str, str] = SLANG_DICT) -> str:\n",
    "    # Replace slang tokens by dictionary expansion (token-by-token).\n",
    "    t = normalize_text_basic(text)\n",
    "    tokens = _token_re.findall(t)\n",
    "    out = []\n",
    "    for tok in tokens:\n",
    "        repl = slang.get(tok.lower())\n",
    "        out.append(repl if repl is not None else tok)\n",
    "    return \" \".join(out)\n",
    "\n",
    "def full_preprocess(text: str) -> str:\n",
    "    # FULL preprocessing used in E4:\n",
    "    # 1) Remove social tokens\n",
    "    # 2) Expand slang\n",
    "    t = remove_social_tokens(text)\n",
    "    t = expand_slang(t, SLANG_DICT)\n",
    "    return t\n",
    "\n",
    "# Quick sanity check\n",
    "sample_text = \"@user lol check this out http://example.com #happy\"\n",
    "print(\"Original:\", sample_text)\n",
    "print(\"Remove social:\", remove_social_tokens(sample_text))\n",
    "print(\"Slang expand:\", expand_slang(sample_text))\n",
    "print(\"FULL:\", full_preprocess(sample_text))"
   ],
   "id": "3a9f2bc17bff5729",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 6) Feature Engineering & Model Architecture\n",
    "\n",
    "### TF‑IDF Vectorization\n",
    "Text data is transformed into sparse numerical vectors using **TF‑IDF (Term Frequency-Inverse Document Frequency)**. We utilize word n-grams (1-gram to 2-gram) to capture local context.\n",
    "\n",
    "### Logistic Regression Classifier\n",
    "We employ a **Multiclass Logistic Regression** model.\n",
    "For experiments addressing class imbalance (E3 and E4), the parameter `class_weight=\"balanced\"` is enabled. This adjusts the loss function to penalize misclassification of minority classes more heavily, aiming to improve the **Macro-F1** score."
   ],
   "id": "63dfe57b302640"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# =========================\n",
    "# 6) Train/Test Split (Stratified) + Model Builder\n",
    "# =========================\n",
    "\n",
    "LABEL_MAP = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
    "\n",
    "df_en_num = df_en.copy()\n",
    "df_en_num[\"y\"] = df_en_num[\"label\"].map(LABEL_MAP)\n",
    "\n",
    "# Drop any unexpected labels (safe-guard)\n",
    "df_en_num = df_en_num.dropna(subset=[\"y\"]).reset_index(drop=True)\n",
    "df_en_num[\"y\"] = df_en_num[\"y\"].astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_en_num[\"text\"].values,\n",
    "    df_en_num[\"y\"].values,\n",
    "    test_size=0.2,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=df_en_num[\"y\"].values,\n",
    ")\n",
    "\n",
    "print(\"Train size:\", len(X_train), \"Test size:\", len(X_test))\n",
    "print(\"Train label distribution:\", pd.Series(y_train).value_counts().to_dict())\n",
    "print(\"Test  label distribution:\", pd.Series(y_test).value_counts().to_dict())\n",
    "\n",
    "def build_pipeline(class_weight: Optional[str] = None) -> Pipeline:\n",
    "    # Build TF-IDF + Logistic Regression.\n",
    "    # Parameters are fixed across experiments for fair comparison.\n",
    "    tfidf = TfidfVectorizer(\n",
    "        lowercase=True,\n",
    "        ngram_range=(1, 2),\n",
    "        min_df=2,\n",
    "        max_df=0.95,\n",
    "        token_pattern=r\"(?u)\\b\\w+\\b\",\n",
    "    )\n",
    "\n",
    "    lr = LogisticRegression(\n",
    "        solver=\"saga\",\n",
    "        max_iter=2000,\n",
    "        C=1.0,\n",
    "        class_weight=class_weight,\n",
    "        n_jobs=-1,\n",
    "        multi_class=\"auto\",\n",
    "    )\n",
    "\n",
    "    return Pipeline([(\"tfidf\", tfidf), (\"lr\", lr)])"
   ],
   "id": "1d6108fa704bc9cd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 7) Execution of Ablation Experiments (E0–E4)\n",
    "\n",
    "Each experiment is evaluated based on:\n",
    "- **Accuracy**: Overall correctness.\n",
    "- **Macro-F1**: The unweighted mean of F1 scores for each class (critical for imbalanced datasets).\n",
    "- **Confusion Matrix**: Visualizing misclassifications.\n",
    "\n",
    "We also compute the **$\\Delta$(improvement)** to quantify the improvement of each method relative to the baseline (E0)."
   ],
   "id": "87b00756aeb42dcb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# =========================\n",
    "# 7) Experiment Runner\n",
    "# =========================\n",
    "\n",
    "TextTransform = Callable[[str], str]\n",
    "\n",
    "def apply_transform(texts: np.ndarray, transform: Optional[TextTransform]) -> List[str]:\n",
    "    # Apply a text transform (if provided) to an array of texts.\n",
    "    # We always apply normalize_text_basic for stable vectorization.\n",
    "    if transform is None:\n",
    "        return [normalize_text_basic(t) for t in texts]\n",
    "    return [transform(t) for t in texts]\n",
    "\n",
    "def evaluate_experiment(\n",
    "    exp_id: str,\n",
    "    name: str,\n",
    "    transform: Optional[TextTransform],\n",
    "    class_weight: Optional[str],\n",
    ") -> Dict:\n",
    "    # Train and evaluate one experiment, returning metrics and confusion matrix.\n",
    "    Xtr = apply_transform(X_train, transform)\n",
    "    Xte = apply_transform(X_test, transform)\n",
    "\n",
    "    model = build_pipeline(class_weight=class_weight)\n",
    "    model.fit(Xtr, y_train)\n",
    "\n",
    "    y_pred = model.predict(Xte)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    macro = f1_score(y_test, y_pred, average=\"macro\")\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    return {\n",
    "        \"exp_id\": exp_id,\n",
    "        \"name\": name,\n",
    "        \"accuracy\": float(acc),\n",
    "        \"macro_f1\": float(macro),\n",
    "        \"confusion_matrix\": cm,\n",
    "        \"y_pred\": y_pred,\n",
    "    }\n",
    "\n",
    "def plot_confusion_matrix(cm: np.ndarray, title: str) -> None:\n",
    "    # Plot confusion matrix using matplotlib only.\n",
    "    plt.figure()\n",
    "    plt.imshow(cm, interpolation=\"nearest\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.colorbar()\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, str(cm[i, j]), ha=\"center\", va=\"center\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Define experiments (E0–E4)\n",
    "experiments = [\n",
    "    (\"E0\", \"Raw (English subset)\", None, None),\n",
    "    (\"E1\", \"Raw + remove @/#/url tokens\", remove_social_tokens, None),\n",
    "    (\"E2\", \"Raw + slang expansion\", lambda t: expand_slang(t, SLANG_DICT), None),\n",
    "    (\"E3\", \"Raw + class_weight=balanced\", None, \"balanced\"),\n",
    "    (\"E4\", \"FULL: remove @/#/url + slang + class_weight\", full_preprocess, \"balanced\"),\n",
    "]\n",
    "\n",
    "results = []\n",
    "cms = {}\n",
    "\n",
    "for exp_id, name, transform, cw in experiments:\n",
    "    print(f\"Running {exp_id}: {name}\")\n",
    "    out = evaluate_experiment(exp_id, name, transform, cw)\n",
    "    results.append({k: out[k] for k in [\"exp_id\", \"name\", \"accuracy\", \"macro_f1\"]})\n",
    "    cms[exp_id] = out[\"confusion_matrix\"]\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Improvements vs E0 baseline\n",
    "base_acc = float(results_df.loc[results_df[\"exp_id\"] == \"E0\", \"accuracy\"].iloc[0])\n",
    "base_f1  = float(results_df.loc[results_df[\"exp_id\"] == \"E0\", \"macro_f1\"].iloc[0])\n",
    "results_df[\"delta_accuracy_vs_E0\"] = results_df[\"accuracy\"] - base_acc\n",
    "results_df[\"delta_macro_f1_vs_E0\"] = results_df[\"macro_f1\"] - base_f1\n",
    "\n",
    "results_df"
   ],
   "id": "dd7eb8767d061096",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 8) Results Analysis\n",
    "\n",
    "We interpret metrics as:\n",
    "- **Accuracy**: overall correctness (may be influenced by majority classes)\n",
    "- **Macro-F1**: average F1 across classes, treating each class equally\n",
    "\n",
    "We visualize confusion matrices for:\n",
    "- Baseline **E0**\n",
    "- Full pipeline **E4**"
   ],
   "id": "afd409b9f1ce06d2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "display(results_df.sort_values(\"exp_id\").reset_index(drop=True))\n",
    "\n",
    "plot_confusion_matrix(cms[\"E0\"], \"Confusion Matrix - E0 (Raw)\")\n",
    "plot_confusion_matrix(cms[\"E4\"], \"Confusion Matrix - E4 (FULL)\")"
   ],
   "id": "9f8398d46f563aab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "## 9) Optional: Error Analysis (Inspect Misclassifications)\n",
    "\n",
    "To better understand *why* preprocessing helps, we can inspect a few misclassified examples\n",
    "(using the FULL pipeline E4).\n"
   ],
   "id": "19c5e4bbbcbf4db1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Re-run E4 to explicitly obtain predictions (kept explicit for clarity)\n",
    "e4_pred = evaluate_experiment(\n",
    "    exp_id=\"E4\",\n",
    "    name=\"FULL: remove @/#/url + slang + class_weight\",\n",
    "    transform=full_preprocess,\n",
    "    class_weight=\"balanced\",\n",
    ")[\"y_pred\"]\n",
    "\n",
    "inv_label_map = {v: k for k, v in LABEL_MAP.items()}\n",
    "\n",
    "err_df = pd.DataFrame({\n",
    "    \"text\": X_test,\n",
    "    \"y_true\": y_test,\n",
    "    \"y_pred\": e4_pred,\n",
    "})\n",
    "err_df[\"true_label\"] = err_df[\"y_true\"].map(inv_label_map)\n",
    "err_df[\"pred_label\"] = err_df[\"y_pred\"].map(inv_label_map)\n",
    "\n",
    "errors = err_df[err_df[\"y_true\"] != err_df[\"y_pred\"]].copy()\n",
    "print(\"Number of errors (E4):\", len(errors))\n",
    "\n",
    "# Show a few random errors\n",
    "errors.sample(min(10, len(errors)), random_state=RANDOM_STATE)[[\"true_label\", \"pred_label\", \"text\"]]"
   ],
   "id": "83509937b82dba05",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 10) Conclusion\n",
    "\n",
    "This notebook presents a reproducible pipeline for evaluating the impact of **text preprocessing** on a **Logistic Regression** sentiment classification model.\n",
    "\n",
    "### Key Findings & Takeaways:\n",
    "1.  **Baseline Performance**: TF-IDF combined with Logistic Regression provides a robust and interpretable baseline for text sentiment analysis.\n",
    "2.  **Incremental Gains**: Individual preprocessing steps (noise removal, slang expansion) contribute to performance improvements.\n",
    "3.  **Optimal Configuration**: The **FULL Pipeline (E4)** typically yields the most significant performance boost, particularly in **Macro-F1** score, demonstrating the synergy between text normalization and class balancing.\n",
    "4.  **Methodological Rigor**: By encapsulating transformations within a pipeline and applying them strictly after the Train-Test split, we ensure zero data leakage and reliable evaluation metrics."
   ],
   "id": "86506931473eda02"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
